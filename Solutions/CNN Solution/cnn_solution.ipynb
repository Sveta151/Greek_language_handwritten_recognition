{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcwaH6M9pqHP"
   },
   "source": [
    "# Handwritten text recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35532,
     "status": "ok",
     "timestamp": 1637224373249,
     "user": {
      "displayName": "Khurshid Juraev",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11990314413528767357"
     },
     "user_tz": -300
    },
    "id": "ucjr4FD5pqHW",
    "outputId": "ca7972a1-b07a-4d44-dd42-b1afcde96375"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n",
      "/content/gdrive/My Drive/CS5242 Project/CNN Solution/\n",
      "/content/gdrive/.shortcut-targets-by-id/1fYoPc4cTre2OE-DPuGBLLQYb7zuw8sxY/CS5242 Project/CNN Solution\n"
     ]
    }
   ],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    # find automatically the path of the folder containing \"file_name\" :\n",
    "    import subprocess\n",
    "    file_name = \"cnn_solution.ipynb\"\n",
    "    # path_to_file = subprocess.check_output('find . -type f -name ' + str(file_name), shell=True).decode(\"utf-8\")\n",
    "    # path_to_file = path_to_file.replace(file_name,\"\").replace('\\n',\"\")\n",
    "    # if previous search failed or too long, comment the previous line and simply write down manually the path below :\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5242 Project/Solutions/CNN Solution/'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"file_name\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rxjtc6TwpqHc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from random import randint\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXsmxFMvpqHd"
   },
   "source": [
    "### Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1637216966078,
     "user": {
      "displayName": "Khurshid Juraev",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11990314413528767357"
     },
     "user_tz": -300
    },
    "id": "HRe9dnL2pqHe",
    "outputId": "ed63a889-364f-4257-e0ec-74aa4b11a2c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device= torch.device(\"cuda\")\n",
    "# device= torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRP34AxxpqHg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "035Bi5DDpqHi"
   },
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6t3ttw2fpqHj"
   },
   "outputs": [],
   "source": [
    "train_data_folder = '../../ready_data/2/'\n",
    "\n",
    "train_data_file = train_data_folder + 'data.pt'\n",
    "train_labels_file = train_data_folder + 'labels.pt'\n",
    "\n",
    "train_data = 1 - torch.load(train_data_file) / 255\n",
    "train_labels = torch.load(train_labels_file)\n",
    "\n",
    "\n",
    "test_data_folder = '../../ready_data/1/'\n",
    "\n",
    "test_data_file = test_data_folder + 'data.pt'\n",
    "test_labels_file = test_data_folder + 'labels.pt'\n",
    "\n",
    "test_data = 1 - torch.load(test_data_file) / 255\n",
    "test_labels = torch.load(test_labels_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvV0OrpsEWmR"
   },
   "source": [
    "### Augment the training data. \n",
    "\n",
    "Apply rotation, translation, scaling and color transformations for each image 20 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqxibRLfpU1-"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "                    [\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.RandomAffine(degrees=20, translate=(0.1,0.1), scale=(0.9, 1.1)),\n",
    "                    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                    transforms.ToTensor(),\n",
    "                    ])\n",
    "\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "for tensor, label in zip(train_data, train_labels):\n",
    "    \n",
    "    for i in range(60):\n",
    "        new_image = train_transform(1 - tensor)\n",
    "        augmented_images.append(1 - new_image)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "train_augmented_data = torch.stack(augmented_images)\n",
    "train_augmented_labels = torch.stack(augmented_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8iCV8S7pqHp"
   },
   "source": [
    "### Make a convnet-based class. \n",
    "\n",
    "Three-layer Convnet and two linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8K5W5WkpqHs"
   },
   "outputs": [],
   "source": [
    "class HTR_convnet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(HTR_convnet, self).__init__()\n",
    "\n",
    "        # CL1:   64 x 64  -->    64 x 64 x 64 \n",
    "        self.conv1 = nn.Conv2d(1,   64,  kernel_size=3,  padding=1 )\n",
    "        \n",
    "        # MP1: 64 x 64 x 64 -->    64 x 32 x 32 \n",
    "        self.pool1  = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        # CL2:   64 x 32 x 32  -->    32 x 32 x 32 \n",
    "        self.conv2 = nn.Conv2d(64, 32,  kernel_size=3,  padding=1 )\n",
    "        \n",
    "        # MP2: 32 x 32 x 32 -->    32 x 16 x 16\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # CL3:   32 x 16 x 16  -->    16 x 16 x 16 \n",
    "        self.conv3 = nn.Conv2d(32, 16, kernel_size=3,  padding=1 )\n",
    "        \n",
    "        # MP3: 16 x 16 x 16 -->  16 x 8 x 8\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # LL1:   16 x 8 x 8 = 1024 -->  100 \n",
    "        self.linear1 = nn.Linear(1024, 100)\n",
    "        \n",
    "        # LL2:   100  -->  48\n",
    "        self.linear2 = nn.Linear(100, 48)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.25)        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # CL1:   64 x 64  -->    64 x 64 x 64 \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        \n",
    "        # MP1: 64 x 64 x 64 -->    64 x 32 x 32 \n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "\n",
    "        # CL2:   64 x 32 x 32  -->    32 x 32 x 32 \n",
    "        x = self.conv2(x)\n",
    "        x = torch.tanh(x)\n",
    "        \n",
    "        # MP2: 32 x 32 x 32 -->    32 x 16 x 16\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # CL3:   32 x 16 x 16  -->    16 x 16 x 16 \n",
    "        x = self.conv3(x)\n",
    "        x = torch.tanh(x)\n",
    "        \n",
    "        # MP3: 16 x 16 x 16 -->  16 x 8 x 8\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # LL1:   16 x 8 x 8 = 1024 -->  100 \n",
    "        x = x.view(-1, 1024)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # LL2:   100  -->  48\n",
    "        x = self.linear2(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy8JgVm3pqHt"
   },
   "source": [
    "### Build the net. How many parameters in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1637217018750,
     "user": {
      "displayName": "Khurshid Juraev",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11990314413528767357"
     },
     "user_tz": -300
    },
    "id": "4lZvak0-pqHu",
    "outputId": "36f9a61b-8700-48d3-a4db-b32b9a43cabe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTR_convnet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear1): Linear(in_features=1024, out_features=100, bias=True)\n",
      "  (linear2): Linear(in_features=100, out_features=48, bias=True)\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout(p=0.25, inplace=False)\n",
      ")\n",
      "There are 131076 (0.13 million) parameters in this neural network\n"
     ]
    }
   ],
   "source": [
    "net=HTR_convnet()\n",
    "print(net)\n",
    "utils.display_num_param(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4f_F-EWpqHv"
   },
   "source": [
    "### Send the weights of the networks to the GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ie6t4ieNpqHw"
   },
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfBotabApqHx"
   },
   "source": [
    "### Choose the criterion, learning rate, and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ddJ1TDBpqHy"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "my_lr=0.3\n",
    "\n",
    "bs= 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGtlAo4apqHy"
   },
   "source": [
    "### Function to evaluate the network on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_na2QFyZpqHz"
   },
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "    for i in range(0,test_data.shape[0],bs):\n",
    "\n",
    "        minibatch_data =  test_data[i:i+bs].unsqueeze(dim=1)\n",
    "        minibatch_label= test_labels[i:i+bs]\n",
    "\n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "        \n",
    "        inputs = minibatch_data\n",
    "\n",
    "        scores=net( inputs )\n",
    "\n",
    "        error = utils.get_error( scores , minibatch_label)\n",
    "\n",
    "        running_error += error.item()\n",
    "\n",
    "        num_batches+=1\n",
    "\n",
    "\n",
    "    total_error = running_error/num_batches\n",
    "    print( 'error rate on test set =', total_error*100 ,'percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4nODWitpqH0"
   },
   "source": [
    "### Do 30 passes through the training set. Divide the learning rate by 2 every 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lnZt7sbupqH0",
    "outputId": "8297b7c9-3d22-4f19-81d1-11b1bb754e4c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 \t time= 0.576301884651184 min \t lr= 0.19999999999999998 \t loss= 2.249062064759893 \t error= 61.76923163865054 percent\n",
      "error rate on test set = 73.16666722297668 percent\n",
      " \n",
      "epoch= 1 \t time= 1.1468231638272604 min \t lr= 0.19999999999999998 \t loss= 0.8378577594846056 \t error= 26.283322312136413 percent\n",
      "error rate on test set = 66.73333466053009 percent\n",
      " \n",
      "epoch= 2 \t time= 1.7166314323743184 min \t lr= 0.19999999999999998 \t loss= 0.5470426089079982 \t error= 17.54112959089664 percent\n",
      "error rate on test set = 67.40000009536743 percent\n",
      " \n",
      "epoch= 3 \t time= 2.2872519214948017 min \t lr= 0.19999999999999998 \t loss= 0.4554595733565042 \t error= 14.5030489198506 percent\n",
      "error rate on test set = 64.10000085830688 percent\n",
      " \n",
      "epoch= 4 \t time= 2.857238761583964 min \t lr= 0.19999999999999998 \t loss= 0.41265225469475925 \t error= 12.977915657265715 percent\n",
      "error rate on test set = 61.30000114440918 percent\n",
      " \n",
      "epoch= 5 \t time= 3.42788698275884 min \t lr= 0.13333333333333333 \t loss= 0.21163742203600222 \t error= 7.003048686516348 percent\n",
      "error rate on test set = 62.533335089683526 percent\n",
      " \n",
      "epoch= 6 \t time= 3.998469809691111 min \t lr= 0.13333333333333333 \t loss= 0.15010857403630168 \t error= 5.108913055196211 percent\n",
      "error rate on test set = 60.10000169277191 percent\n",
      " \n",
      "epoch= 7 \t time= 4.569669504960378 min \t lr= 0.13333333333333333 \t loss= 0.13254964652897538 \t error= 4.536178946404265 percent\n",
      "error rate on test set = 62.73333430290222 percent\n",
      " \n",
      "epoch= 8 \t time= 5.140016198158264 min \t lr= 0.13333333333333333 \t loss= 0.11274499040996402 \t error= 3.846536882034441 percent\n",
      "error rate on test set = 65.13333320617676 percent\n",
      " \n",
      "epoch= 9 \t time= 5.71082102060318 min \t lr= 0.13333333333333333 \t loss= 0.10040321413435394 \t error= 3.418128597582013 percent\n",
      "error rate on test set = 62.13333427906036 percent\n",
      " \n",
      "epoch= 10 \t time= 6.281344318389893 min \t lr= 0.08888888888888889 \t loss= 0.061397092133602275 \t error= 2.0167575841265966 percent\n",
      "error rate on test set = 60.30000030994416 percent\n",
      " \n",
      "epoch= 11 \t time= 6.852602791786194 min \t lr= 0.08888888888888889 \t loss= 0.04215553740274604 \t error= 1.3937563816282645 percent\n",
      "error rate on test set = 63.13333332538604 percent\n",
      " \n",
      "epoch= 12 \t time= 7.423273181915283 min \t lr= 0.08888888888888889 \t loss= 0.04018607267467816 \t error= 1.3198798608888858 percent\n",
      "error rate on test set = 62.933332324028015 percent\n",
      " \n",
      "epoch= 13 \t time= 7.99438705444336 min \t lr= 0.08888888888888889 \t loss= 0.03284004987818625 \t error= 1.0251346791144644 percent\n",
      "error rate on test set = 60.53333401679992 percent\n",
      " \n",
      "epoch= 14 \t time= 8.564827410380046 min \t lr= 0.08888888888888889 \t loss= 0.028536805035292843 \t error= 0.8808085778607545 percent\n",
      "error rate on test set = 60.53333461284638 percent\n",
      " \n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "N = train_augmented_data.shape[0]\n",
    "\n",
    "for epoch in range(50):\n",
    "    \n",
    "    if not epoch % 5:\n",
    "        my_lr = my_lr / 1.5\n",
    "        \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "        \n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "    \n",
    "    shuffled_indices=torch.randperm(N)\n",
    " \n",
    "    for count in range(0,N,bs):\n",
    "        \n",
    "        # FORWARD AND BACKWARD PASS\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "             \n",
    "        indices=shuffled_indices[count:count+bs]\n",
    "        minibatch_data =  train_augmented_data[indices]\n",
    "        minibatch_label=  train_augmented_labels[indices]\n",
    "        \n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "        \n",
    "        inputs = minibatch_data\n",
    "        \n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        scores=net( inputs ) \n",
    "        \n",
    "        loss =  criterion( scores , minibatch_label) \n",
    "          \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # COMPUTE STATS\n",
    "        \n",
    "        running_loss += loss.detach().item()\n",
    "        \n",
    "        error = utils.get_error( scores.detach() , minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches+=1        \n",
    "    \n",
    "    \n",
    "    # AVERAGE STATS THEN DISPLAY\n",
    "    total_loss = running_loss/num_batches\n",
    "    total_error = running_error/num_batches\n",
    "    elapsed = (time.time()-start)/60\n",
    "    \n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'min', '\\t lr=', my_lr  ,'\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "    eval_on_test_set() \n",
    "    print(' ')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5b0db5MpqH2"
   },
   "source": [
    "### Choose image at random from the test set and see how good/bad are the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ED84-HRNCHQv"
   },
   "outputs": [],
   "source": [
    "label = ['Alpha', 'Beta', 'Gamma', 'Delta', 'Epsilon', 'Zeta', 'Eta', 'Theta', 'Iota', 'Kappa', 'Lambda', 'Mu', 'Nu', 'Xi', 'Omicron', 'Pi', 'Rho', 'Sigma', 'Tau', 'Upsilon', 'Phi', 'Chi', 'Psi', 'Omega', 'alpha', 'beta', 'gamma', 'delta', 'epsilon', 'zeta', 'eta', 'theta', 'iota', 'kappa', 'lambda', 'mu', 'nu', 'xi', 'omicron', 'pi', 'rho', 'sigma', 'tau', 'upsilon', 'phi', 'chi', 'psi', 'omega']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F55X_237CHQ4"
   },
   "outputs": [],
   "source": [
    "# choose a picture at random\n",
    "idx=randint(0, 400)\n",
    "im=test_data[idx]\n",
    "\n",
    "answer = label[test_labels[idx]]\n",
    "\n",
    "print(answer)\n",
    "\n",
    "# diplay the picture\n",
    "utils.show(im)\n",
    "\n",
    "# send to device, rescale, and view as a batch of 1 \n",
    "im = im.to(device)\n",
    "im = im\n",
    "im = im.view(1,64,64).unsqueeze(dim=1)\n",
    "\n",
    "# feed it to the net and display the confidence scores\n",
    "scores =  net(im) \n",
    "probs= F.softmax(scores, dim=1)\n",
    "utils.show_prob_greek(probs.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RC1pz2ZCM4f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
