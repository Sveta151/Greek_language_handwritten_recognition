{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcwaH6M9pqHP"
   },
   "source": [
    "# Handwritten text recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16841,
     "status": "ok",
     "timestamp": 1637133876381,
     "user": {
      "displayName": "Aristotelis Mitsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVYgNEstM7OTRfF0hE2kfTWJ6KfF93l6HdKmTn=s64",
      "userId": "09910844015288244447"
     },
     "user_tz": -480
    },
    "id": "ucjr4FD5pqHW",
    "outputId": "ca3ae09b-e0d2-4479-c497-7a9547bceceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n",
      "/content/gdrive/My Drive/CS5242 Project/RNN Solution [Aristotelis]/\n",
      "/content/gdrive/.shortcut-targets-by-id/1fYoPc4cTre2OE-DPuGBLLQYb7zuw8sxY/CS5242 Project/RNN Solution [Aristotelis]\n"
     ]
    }
   ],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    # find automatically the path of the folder containing \"file_name\" :\n",
    "    file_name = 'rnn_solution.ipynb'\n",
    "    import subprocess\n",
    "    # path_to_file = subprocess.check_output('find . -type f -name ' + str(file_name), shell=True).decode(\"utf-8\")\n",
    "    # path_to_file = path_to_file.replace(file_name,\"\").replace('\\n',\"\")\n",
    "    # if previous search failed or too long, comment the previous line and simply write down manually the path below :\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5242 Project/Solutions/RNN Solution/'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"file_name\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rxjtc6TwpqHc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXsmxFMvpqHd"
   },
   "source": [
    "### With or without GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1637133909180,
     "user": {
      "displayName": "Aristotelis Mitsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVYgNEstM7OTRfF0hE2kfTWJ6KfF93l6HdKmTn=s64",
      "userId": "09910844015288244447"
     },
     "user_tz": -480
    },
    "id": "HRe9dnL2pqHe",
    "outputId": "34817422-32e7-4f54-a536-1264001210d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device= torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRP34AxxpqHg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "035Bi5DDpqHi"
   },
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6t3ttw2fpqHj"
   },
   "outputs": [],
   "source": [
    "train_data_folder = '../../ready_data/2/'\n",
    "\n",
    "train_data_file = train_data_folder + 'data.pt'\n",
    "train_labels_file = train_data_folder + 'labels.pt'\n",
    "\n",
    "train_data = torch.load(train_data_file) / 255\n",
    "train_labels = torch.load(train_labels_file)\n",
    "\n",
    "\n",
    "test_data_folder = '../../ready_data/1/'\n",
    "\n",
    "test_data_file = test_data_folder + 'data.pt'\n",
    "test_labels_file = test_data_folder + 'labels.pt'\n",
    "\n",
    "test_data = torch.load(test_data_file) / 255\n",
    "test_labels = torch.load(test_labels_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pVnLSP-pqHk"
   },
   "source": [
    "### Compute mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1637133965281,
     "user": {
      "displayName": "Aristotelis Mitsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVYgNEstM7OTRfF0hE2kfTWJ6KfF93l6HdKmTn=s64",
      "userId": "09910844015288244447"
     },
     "user_tz": -480
    },
    "id": "RfKgBNU4pqHl",
    "outputId": "cbc4e5dd-f16b-4a1b-a896-b6d308b9e49d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9515)\n"
     ]
    }
   ],
   "source": [
    "train_mean= train_data.mean()\n",
    "\n",
    "print(train_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wq45zhAipqHm"
   },
   "source": [
    "### Compute standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1637133967144,
     "user": {
      "displayName": "Aristotelis Mitsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVYgNEstM7OTRfF0hE2kfTWJ6KfF93l6HdKmTn=s64",
      "userId": "09910844015288244447"
     },
     "user_tz": -480
    },
    "id": "n9bCd_v3pqHn",
    "outputId": "aa1eda9d-1f4e-4fa7-e4a4-95a55113fd65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1913)\n"
     ]
    }
   ],
   "source": [
    "train_std= train_data.std()\n",
    "\n",
    "print(train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqxibRLfpU1-"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "                    [\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.RandomAffine(degrees=20, translate=(0.1,0.1), scale=(0.9, 1.1)),\n",
    "                    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                    transforms.ToTensor(),\n",
    "                    ])\n",
    "\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "for tensor, label in zip(train_data, train_labels):\n",
    "    \n",
    "    for i in range(60):\n",
    "        new_image = train_transform(1 - tensor)\n",
    "        augmented_images.append(1 - new_image)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "train_augmented_data = torch.stack(augmented_images)\n",
    "train_augmented_labels = torch.stack(augmented_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x25lw6ZWSe5E"
   },
   "source": [
    "### Some constants associated with the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzE0KFQXSe5E"
   },
   "outputs": [],
   "source": [
    "bs = 20\n",
    "\n",
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8iCV8S7pqHp"
   },
   "source": [
    "### Make a RNN class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8K5W5WkpqHs"
   },
   "outputs": [],
   "source": [
    "class handwriting_RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(handwriting_RNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Embedding( vocab_size  , hidden_size  )\n",
    "        self.layer2 = nn.RNN(       hidden_size , hidden_size  )\n",
    "        self.layer3 = nn.Linear(    hidden_size , vocab_size   )\n",
    "\n",
    "        \n",
    "    def forward(self, word_seq, h_init ):\n",
    "        \n",
    "        g_seq               =   self.layer1( word_seq )  \n",
    "        h_seq , h_final     =   self.layer2( g_seq , h_init )\n",
    "        score_seq           =   self.layer3( h_seq )\n",
    "        \n",
    "        return score_seq,  h_final "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy8JgVm3pqHt"
   },
   "source": [
    "### Build the net. How many parameters in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1637134046475,
     "user": {
      "displayName": "Aristotelis Mitsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVYgNEstM7OTRfF0hE2kfTWJ6KfF93l6HdKmTn=s64",
      "userId": "09910844015288244447"
     },
     "user_tz": -480
    },
    "id": "4lZvak0-pqHu",
    "outputId": "3046e3dc-7757-4edb-8319-634d923ed420"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handwriting_RNN(\n",
      "  (layer1): Embedding(10000, 150)\n",
      "  (layer2): RNN(150, 150)\n",
      "  (layer3): Linear(in_features=150, out_features=10000, bias=True)\n",
      ")\n",
      "There are 3055300 (3.06 million) parameters in this neural network\n"
     ]
    }
   ],
   "source": [
    "hidden_size=150\n",
    "net = handwriting_RNN(hidden_size)\n",
    "print(net)\n",
    "utils.display_num_param(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4f_F-EWpqHv"
   },
   "source": [
    "### Send the weights of the networks to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ie6t4ieNpqHw"
   },
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bx6vbKe7Try8"
   },
   "source": [
    "### Set up manually the weights of the embedding module and Linear module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1637134082778,
     "user": {
      "displayName": "Aristotelis Mitsiou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVYgNEstM7OTRfF0hE2kfTWJ6KfF93l6HdKmTn=s64",
      "userId": "09910844015288244447"
     },
     "user_tz": -480
    },
    "id": "nUyA4us0TswO",
    "outputId": "cf35f581-7fd1-407e-ea67-0181ec536767"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net.layer1.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "net.layer3.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WW25g49WSe5G"
   },
   "source": [
    "### Choose the criterion, as well as the following important hyperparameters: \n",
    "* initial learning rate = 1\n",
    "* sequence length = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EPF1TaGZSe5G"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "my_lr = 1\n",
    "\n",
    "seq_length = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGtlAo4apqHy"
   },
   "source": [
    "### Function to evaluate the network on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_na2QFyZpqHz"
   },
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "    \n",
    "    h=h.to(device)\n",
    "\n",
    "       \n",
    "    for count in range( 0 , 4120-seq_length ,  seq_length) :\n",
    "               \n",
    "        minibatch_data =  test_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = test_data[ count+1 : count+seq_length+1 ]\n",
    "        \n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "                                  \n",
    "        scores, h  = net( minibatch_data, h )\n",
    "        \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length ) \n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)\n",
    "        \n",
    "        loss = criterion(  scores ,  minibatch_label )    \n",
    "        \n",
    "        h=h.detach()\n",
    "            \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1        \n",
    "    \n",
    "    total_loss = running_loss/num_batches \n",
    "    print('test: exp(loss) = ', math.exp(total_loss)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4nODWitpqH0"
   },
   "source": [
    "### Do 10 passes through the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnZt7sbupqH0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    # keep the learning rate to 1 during the first 4 epochs, then divide by 1.1 at every epoch\n",
    "    if epoch >= 4:\n",
    "        my_lr = my_lr / 1.1\n",
    "    \n",
    "    # create a new optimizer and give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "        \n",
    "    # set the running quantities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    # set the initial h to be the zero vector\n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "\n",
    "    # send it to the gpu    \n",
    "    h=h.to(device)\n",
    "    \n",
    "    for count in range( 0 , 46478-seq_length ,  seq_length):\n",
    "             \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch\n",
    "        minibatch_data =  train_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = train_data[ count+1 : count+seq_length+1 ]        \n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "        \n",
    "        # Detach to prevent from backpropagating all the way to the beginning\n",
    "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
    "        h=h.detach()\n",
    "        h=h.requires_grad_()\n",
    "                       \n",
    "        # forward the minibatch through the net        \n",
    "        scores, h  = net( minibatch_data, h )\n",
    "        \n",
    "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)  \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length )       \n",
    "        \n",
    "        # Compute the average of the losses of the data points in this huge batch\n",
    "        loss = criterion(  scores ,  minibatch_label )\n",
    "        \n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "        utils.normalize_gradient(net)\n",
    "        optimizer.step()\n",
    "        \n",
    "            \n",
    "        # update the running loss  \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "    print('')\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
    "    eval_on_test_set() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5b0db5MpqH2"
   },
   "source": [
    "### Choose image at random from the test set and see how good/bad are the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Cvq1J4PpU2K"
   },
   "outputs": [],
   "source": [
    "label = ['Alpha', 'Beta', 'Gamma', 'Delta', 'Epsilon', 'Zeta', 'Eta', 'Theta', 'Iota', 'Kappa', 'Lambda', 'Mu', 'Nu', 'Xi', 'Omicron', 'Pi', 'Rho', 'Sigma', 'Tau', 'Upsilon', 'Phi', 'Chi', 'Psi', 'Omega', 'alpha', 'beta', 'gamma', 'delta', 'epsilon', 'zeta', 'eta', 'theta', 'iota', 'kappa', 'lambda', 'mu', 'nu', 'xi', 'omicron', 'pi', 'rho', 'sigma', 'tau', 'upsilon', 'phi', 'chi', 'psi', 'omega']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgBrqPuTpqH2"
   },
   "outputs": [],
   "source": [
    "# choose a picture at random\n",
    "idx=randint(0, 400)\n",
    "im=test_data[idx]\n",
    "\n",
    "answer = label[test_labels[idx]]\n",
    "\n",
    "print(answer)\n",
    "\n",
    "# diplay the picture\n",
    "utils.show(im)\n",
    "\n",
    "# send to device, rescale, and view as a batch of 1 \n",
    "im = im.to(device)\n",
    "im= (im-train_mean) / train_std\n",
    "im=im.view(1,64,64).unsqueeze(dim=1)\n",
    "\n",
    "# feed it to the net and display the confidence scores\n",
    "scores =  net(im) \n",
    "probs= F.softmax(scores, dim=1)\n",
    "utils.show_prob_greek(probs.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Talg0yGEpqH3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "rnn_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
